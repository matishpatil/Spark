You can launch spark 2 either by exporting SPARK_MAJOR_VERSION to 2 or by using spark2 related commands.
    - spark2-shell(Spark 2 with Scala)
    - pyspark2(Spark 2 with python)
    - spark2-submit(Spark 2 submit command to submit Spark 2 jobs)
    
minPartitions value cannot be changed even though the minPartitions value specified is less than the actual minPartitions but can be changed if its greater than minPartitions value.

export PYSPARK_PYTHON=/usr/local/bin/python3
export PYSPARK_DRIVER_PYTHON=python3
pyspark2 --master yarn --conf spark.dynamicAllocation.enabled=false --conf spark.ui.port=12342
=========================================================================
data = sc.textFile('/public/randomtextwriter/part-m-00000')
wc = data. \
  flatMap(lambda line: line.split(' ')). \
  map(lambda word: (word, 1)). \
  reduceByKey(lambda x, y: x + y)with
wc.map(lambda rec: rec[0] + ',' + str(rec[1])).saveAsTextFile('/user/matishpatil/core/wordcount')
=========================================================================
from pyspark.sql.functions import split, explode
data = spark.read.text('/public/randomtextwriter/part-m-00000')
wc = data.select(explode(split(data.value, ' ')).alias('words')). \
  groupBy('words').count()
  ####agg(count('words').alias('wc'))
wc.write.csv('/user/matishpatil/df/wordcount')
=========================================================================
ordersDF = spark.read.csv("/user/matishpatil/retail_db/orders")
ordersDF.select('_c0','_c1').show()
ordersDF.describe().show()

ordersDF2 = spark.read.json("/user/matishpatil/retail_db_json/orders")
ordersDF2.printSchema()

python3 -m pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local').appName('Create Dataframe over JDBC').getOrCreate()
=========================================================================
df.select(df.field.cast(IntegerType()))

spark = SparkSession. \
  builder. \
  master('local'). \
  appName('CSV Example'). \
  getOrCreate()

orders = spark.read. \
  format('csv'). \
  schema('order_id int, order_date string, order_customer_id int, order_status string'). \
  load('/Users/itversity/Research/data/retail_db/orders')
  
orders.printSchema()
orders.show()

orderItems = spark.read. \
  format('csv'). \
  schema('''order_item_id int, 
            order_item_order_id int, 
            order_item_product_id int, 
            order_item_quantity int,
            order_item_subtotal float,
            order_item_product_price float
         '''). \
  load('/Users/itversity/Research/data/retail_db/order_items')

orderItems.printSchema()
orderItems.show()
=========================================================================
# In case you are using pycharm, first you need to create object of type SparkSession
spark = SparkSession. \
  builder. \
  master('local'). \
  appName('CSV Example'). \
  getOrCreate()

ordersCSV = spark.read. \
  csv('/public/retail_db/orders'). \
  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')

orderItemsCSV = spark.read. \
  csv('/public/retail_db/order_items'). \
  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', 
       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')

from pyspark.sql.types import IntegerType, FloatType

orders = ordersCSV. \
  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \
  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))

orders.printSchema()
orders.show()

orderItems = orderItemsCSV.\
    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \
    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \
    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \
    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \
    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \
    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))

orderItems.printSchema()
orderItems.show()
=========================================================================
orders = spark.read.format("csv").option("sep",",").schema('order_id int,order_date string,order_customer_id int,order_status string').load("/public/retail_db/orders")
=========================================================================
data = spark.read.csv('/public/retail_db/orders', header=True)
=========================================================================
orders = spark.read.table("retail.orders")
type(orders)
orders = spark.sql("select * from retail_db.orders")
=========================================================================
mysql -h ms.itversity.com -u retail_user -p
pyspark2 --master yarn --conf spark.ui.port=0 --jars /usr/share/java/mysql-connector-java.jar --driver-class-path /usr/share/java/mysql-connector-java.jar

from pyspark.sql import SparkSession

spark = SparkSession. \
    builder. \
    master('local'). \
    appName('Create Dataframe over JDBC'). \
    getOrCreate()

orders = spark.read. \
  format('jdbc'). \
  option('url', 'jdbc:mysql://ms.itversity.com'). \
  option('dbtable', 'retail_db.orders'). \
  option('user', 'retail_user'). \
  option('password', 'itversity'). \
  load()

orders.show()

orderItems = spark.read. \
    jdbc("jdbc:mysql://ms.itversity.com", "retail_db.order_items",
          properties={"user": "retail_user",
                      "password": "itversity",
                      "numPartitions": "4",
                      "partitionColumn": "order_item_order_id",
                      "lowerBound": "10000",
                      "upperBound": "20000"})

orderItems.write.json('/user/training/bootcamp/pyspark/orderItemsJDBC')

query = "(select order_status, count(1) from retail_db.orders group by order_status) t"
queryData = spark.read. \
    jdbc("jdbc:mysql://ms.itversity.com", query,
         properties={"user": "retail_user",
                     "password": "itversity"})

queryData.show()
=========================================================================
We can get list of tables by using spark.sql('show tables')
We can register data frame as temporary view df.createTempView("view_name")
Output of show tables show the temporary tables as well
Once temp view is created, we can use SQL style syntax and run queries against the tables/views
Most of the hive queries will work out of the box
=========================================================================
spark.sql('describe function substring').show(truncate=False)
=========================================================================

=========================================================================

=========================================================================

=========================================================================

=========================================================================

=========================================================================


=========================================================================

=========================================================================

=========================================================================

=========================================================================

=========================================================================